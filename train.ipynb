{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question: What is the capital of France?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain \n",
    "import keye as k\n",
    "# Hugging Face Hub API setup\n",
    "hf_api_token = k.hf_api_key\n",
    "model_id = \"EleutherAI/gpt-neo-1.3B\"  # Example model\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=model_id, huggingfacehub_api_token=hf_api_token)\n",
    "\n",
    "# Define a prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the question: {question}\",\n",
    ")\n",
    "\n",
    "# Create the LLM chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "question = \"What is the capital of France?\"\n",
    "print(chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_temp = PromptTemplate(\n",
    "    input_variables = [\"name\"],\n",
    "    template = \"we are goint to start a restaurent type {name}  .gave some sugested names\"\n",
    ")\n",
    "name_chain = LLMChain(llm = llm , prompt = name_temp , output_key = \"rest_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prompt = PromptTemplate(\n",
    "    input_variables = [\"res\"],\n",
    "    template = \"gave the recipes / items to add to my restaurent {name}\"\n",
    ")\n",
    "item_chain = LLMChain(llm = llm  , prompt = item_prompt , output_key = \"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain , SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import \n",
    "name_temp = PromptTemplate(\n",
    "    input_variables = [\"name\"],\n",
    "    template = \"we are goint to start a restaurent type {name}  .gave some sugested names\"\n",
    ")\n",
    "item_prompt = PromptTemplate(\n",
    "    input_variables = [\"res\"],\n",
    "    template = \"gave the recipes / items to add to my restaurent {name} . only names comma saparated\"\n",
    ")\n",
    "item_chain = LLMChain(llm = llm  , prompt = item_prompt , output_key = \"items\")\n",
    "name_chain = LLMChain(llm = llm , prompt = name_temp , output_key = \"rest_name\")\n",
    "proces_chain = SequentialChain(\n",
    "    chains= [name_chain , item_chain],\n",
    "    input_variables = [\"name\"],\n",
    "    output_variables = [\"rest_name\" , \"items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.tools import BaseTool\n",
    "\n",
    "class GeminiAPIWrapper(BaseTool):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def _call(self, query: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        url = f\"https://api.gemini.com/v1/{query}\"  # Adjust endpoint as needed\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
